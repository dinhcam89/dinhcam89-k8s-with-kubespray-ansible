[
{
	"uri": "//localhost:1313/vi/1-terraform-for-infra/",
	"title": "Sử dụng Terraform để triển khai Hạ tầng trên AWS",
	"tags": [],
	"description": "",
	"content": "Nội dung:\nCấu trúc các file cấu hình Terraform Các files cấu hình Apply cấu hình lên AWS Cấu trúc các file cấu hình Terraform Vì trong bài LAB này mình sẽ chỉ tập trung vào triển khai Kubernetes Cluster một cách thủ công nên mình sẽ không giải thích quá kĩ về Terraform cũng như Ansible, các bạn muốn tìm hiểu về các công cụ này có thể tìm xem trên các trang như Medium, hoặc ở web Viblo VN mình thấy có rất nhiều anh hướng dẫn rất hay và chi tiết.\nterraform/ │ ├── 0-provider.tf # File cấu hình dùng để định nghĩa Cloud Provider mà bạn sẽ sử dụng file Terraform để deploy ├── 1-vpc.tf # Định nghĩa các cấu hình cho AWS VPC ├── 2-instances.tf # Định nghĩa cấu hình cho các AWS EC2 Instances ├── 3-security-groups.tf # Định nghĩa các security groups ├── 4-load-balanceres.tf # Cấu hình Load Balancing cho các Master Nodeds ├── 5-vars.tf # Định nghĩa các biến dùng trong các file .tf ├── 6-key.tf # Cấu hình tạo SSH key để SSH vào Bastion Instance ├── 7-output.tf # Cấu hình output sau khi apply Các files cấu hình File 0-provider.tf:\nFile này định nghĩa Cloud Provider (trong trường hợp này là AWS) mà Terraform sẽ tương tác với các tài nguyên của nhà cung cấp đó. Nó thiết lập nhà cung cấp cần thiết và chỉ định vùng (region) của AWS nơi các tài nguyên sẽ được triển khai. provider \u0026#34;aws\u0026#34; { region = var.region } terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } } File 1-vpc.tf:\nĐây là file Terraform dùng để triển khai VPC - Virtual Private Cloud, dịch sát nghĩa thì là một \u0026ldquo;mạng riêng ảo\u0026rdquo; trên môi trường AWS, bạn nào nếu đã xem kĩ các video trong module 1,2 có video anh Hưng đã nói rất kĩ về VPC. Với Terraform, các bạn có thể tự định nghĩa các thành phần của một VPC như network, các subnet, internet gateway, natgateway, security group,\u0026hellip; nhưng trong bài LAB lần này mình sẽ sử dụng module có sẵn được public trên registry của Terraform và mình chỉ cần thay đổi giá trị của các thuộc tính theo Document của module để tùy chỉnh VPC tùy ý. Ví dụ như: single_nat_gateway = true: có nghĩa là tất cả các subnet sẽ dùng chung một NAT gateway, theo mặc định thì module này sẽ tự tạo một NAT gateway cho mỗi subnet. cidr: Định nghĩa CIDR block cho VPC, tức là dải địa chỉ IP mà VPC có thể sử dụng. Ví dụ: \u0026ldquo;10.0.0.0/16\u0026rdquo; cho phép bạn sử dụng các địa chỉ IP từ 10.0.0.0 đến 10.0.255.255. azs: Danh sách các vùng khả dụng (Availability Zones) mà bạn sẽ sử dụng để triển khai các subnet trong VPC (như us-east-1a, us-east-1b, us-east-1c,\u0026hellip;). Nhằm đảm bảo khả năng dự phòng cũng như cân bằng tải cho các tài nguyên. Các thuộc tính, tùy chọn khác các bạn có thể tham khảo trên Document của từng module tương ứng ở registry của Terraform module \u0026#34;vpc\u0026#34; { source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; name = \u0026#34;self-managed-k8s-vpc\u0026#34; cidr = \u0026#34;10.0.0.0/16\u0026#34; azs = var.availability_zones private_subnets = var.private_subnets public_subnets = var.public_subnets enable_nat_gateway = true single_nat_gateway = true one_nat_gateway_per_az = false map_public_ip_on_launch = true //assign public ip for BASTION instance tags = { Terraform = \u0026#34;true\u0026#34; Environment = \u0026#34;dev\u0026#34; } } File 2-instances.tf:\nỞ đây mọi người chú ý tạo các Instance có tối thiểu 1 CPU và 2GB RAM nếu không lúc cài đặt K8s spray sẽ gặp lỗi không đủ tài nguyên\nFile này sẽ định nghĩa các cấu hình liên quan đến các instances, bao gồm Bastion Instance,các Master Nodes và các Worker Nodes Như: ami_id :là id của \u0026ldquo;image\u0026rdquo; mà các instance EC2 của bạn sẽ sử dụng, như Linux, Ubuntu, Centos,\u0026hellip; instance_type :là cấu hình của các instance EC2 mà bạn muốn deploy, ví dụ như t2.micro, t2.small, t3.medium, t3.large,\u0026hellip; subnet_id :cấu hình subnet ID nơi mà EC2 instance thuộc về security_groups :cấu hình các ID của security groups mà chúng ta cần gán vào instance key_name :tên của SSH để import vào instance user_data :c :cấu hình import SSH key vào instance Trong bài LAB này, mình sử dụng Terraform để gen ra SSH key rồi import vào Bastion Instance, trên thực tế bạn cũng có thể tự tạo key trên AWS rồi gán vào instance trên AWS hoặc tự tạo key ở trên máy local sau đó import public key vào mục user data của Instance sau khi đã triển khai hạ tầng thành công với Terraform\n// Bastion Instance resource \u0026#34;aws_instance\u0026#34; \u0026#34;bastion\u0026#34; { ami = var.ami_id instance_type = var.bastion_instance_type subnet_id = module.vpc.public_subnets[0] associate_public_ip_address = \u0026#34;true\u0026#34; security_groups = [aws_security_group.allow_ssh.id] key_name = aws_key_pair.k8_ssh.key_name user_data = \u0026lt;\u0026lt;-EOF #!bin/bash echo \u0026#34;PubkeyAcceptedKeyTypes=+ssh-rsa\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config.d/10-insecure-rsa-keysig.conf systemctl reload sshd echo \u0026#34;${tls_private_key.ssh.private_key_pem}\u0026#34; \u0026gt;\u0026gt; /home/ubuntu/.ssh/id_rsa chown ubuntu /home/ubuntu/.ssh/id_rsa chgrp ubuntu /home/ubuntu/.ssh/id_rsa chmod 600 /home/ubuntu/.ssh/id_rsa echo \u0026#34;starting ansible install\u0026#34; apt-add-repository ppa:ansible/ansible -y apt update apt install ansible -y EOF tags = { Name = \u0026#34;Bastion\u0026#34; } } // Master Nodes resource \u0026#34;aws_instance\u0026#34; \u0026#34;masters\u0026#34; { count = var.master_node_count ami = var.ami_id instance_type = var.master_instance_type subnet_id = \u0026#34;${element(module.vpc.private_subnets, count.index)}\u0026#34; key_name = aws_key_pair.k8_ssh.key_name security_groups = [aws_security_group.k8_nondes.id, aws_security_group.k8_masters.id] tags = { Name = format(\u0026#34;k8s-master-%02d\u0026#34;, count.index + 1) } } // Worker Nodes resource \u0026#34;aws_instance\u0026#34; \u0026#34;workers\u0026#34; { count = var.worker_node_count ami = var.ami_id instance_type = var.worker_instance_type subnet_id = \u0026#34;${element(module.vpc.private_subnets, count.index)}\u0026#34; key_name = aws_key_pair.k8_ssh.key_name security_groups = [aws_security_group.k8_nondes.id, aws_security_group.k8_workers.id] tags = { Name = format(\u0026#34;k8s-worker-%02d\u0026#34;, count.index + 1) } } File 3-security_groups.tf:\nPhần này sẽ cấu hình các Security Groups cần cho VPC: SG để cho phép SSH vào Bastion Instance Các SG cần thiết cho Kubernets Cluster, master node, worker node resource \u0026#34;aws_security_group\u0026#34; \u0026#34;allow_ssh\u0026#34; { name = \u0026#34;allow_ssh\u0026#34; description = \u0026#34;Allow ssh inbound traffic\u0026#34; vpc_id = module.vpc.vpc_id ingress { from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } ingress { from_port = -1 to_port = -1 protocol = \u0026#34;icmp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;k8_nondes\u0026#34; { name = \u0026#34;k8_nodes\u0026#34; description = \u0026#34;sec group for k8 nodes\u0026#34; vpc_id = module.vpc.vpc_id ingress { from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } ingress { from_port = -1 to_port = -1 protocol = \u0026#34;icmp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;k8_masters\u0026#34; { name = \u0026#34;k8_masters\u0026#34; description = \u0026#34;sec group for k8 master nodes\u0026#34; vpc_id = module.vpc.vpc_id ingress { #Kubernetes API server from_port = 6443 to_port = 6443 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } ingress { #etcd server client API from_port = 2379 to_port = 2380 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } ingress { #Kubelet API from_port = 10250 to_port = 10250 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } ingress { #kube-scheduler from_port = 10259 to_port = 10259 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } ingress { #kube-controller-manager from_port = 10257 to_port = 10257 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;k8_workers\u0026#34; { name = \u0026#34;k8_workers\u0026#34; description = \u0026#34;sec group for k8 worker nodes\u0026#34; vpc_id = module.vpc.vpc_id ingress { #Kubelet API from_port = 10250 to_port = 10250 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } ingress { #NodePort Services† from_port = 30000 to_port = 32767 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } } File 4-load-balancers.tf:\nresource \u0026#34;aws_lb\u0026#34; \u0026#34;k8_masters_lb\u0026#34; { name = \u0026#34;k8-masters-lb\u0026#34; internal = true load_balancer_type = \u0026#34;network\u0026#34; subnets = module.vpc.private_subnets #[for subnet in module.vpc.private_subnets : subnet.id] tags = { Terraform = \u0026#34;true\u0026#34; Environment = \u0026#34;dev\u0026#34; } } # target_type instance not working well when we bound this LB as a control-plane-endpoint. hence had to use IP target_type #https://stackoverflow.com/questions/56768956/how-to-use-kubeadm-init-configuration-parameter-controlplaneendpoint/70799078#70799078 resource \u0026#34;aws_lb_target_group\u0026#34; \u0026#34;k8_masters_api\u0026#34; { name = \u0026#34;k8-masters-api\u0026#34; port = 6443 protocol = \u0026#34;TCP\u0026#34; vpc_id = module.vpc.vpc_id target_type = \u0026#34;ip\u0026#34; health_check { port = 6443 protocol = \u0026#34;TCP\u0026#34; interval = 30 healthy_threshold = 2 unhealthy_threshold = 2 } } resource \u0026#34;aws_lb_listener\u0026#34; \u0026#34;k8_masters_lb_listener\u0026#34; { load_balancer_arn = aws_lb.k8_masters_lb.arn port = 6443 protocol = \u0026#34;TCP\u0026#34; default_action { target_group_arn = aws_lb_target_group.k8_masters_api.id type = \u0026#34;forward\u0026#34; } } resource \u0026#34;aws_lb_target_group_attachment\u0026#34; \u0026#34;k8_masters_attachment\u0026#34; { count = length(aws_instance.masters.*.id) target_group_arn = aws_lb_target_group.k8_masters_api.arn target_id = aws_instance.masters.*.private_ip[count.index] } File 5-vars.tf:\nvariable \u0026#34;region\u0026#34; { default = \u0026#34;us-east-1\u0026#34; } variable \u0026#34;ami_id\u0026#34; { type = string default = \u0026#34;ami-04a81a99f5ec58529\u0026#34; // Ubuntu 24.04 LTS } variable \u0026#34;availability_zones\u0026#34; { type = list(string) default = [\u0026#34;us-east-1a\u0026#34;, \u0026#34;us-east-1b\u0026#34;, \u0026#34;us-east-1c\u0026#34;] } variable \u0026#34;vpc_cidr\u0026#34; { type = string default = \u0026#34;10.0.0.0/16\u0026#34; } variable \u0026#34;private_subnets\u0026#34; { type = list(string) default = [\u0026#34;10.0.1.0/24\u0026#34;, \u0026#34;10.0.2.0/24\u0026#34;, \u0026#34;10.0.3.0/24\u0026#34;] } variable \u0026#34;public_subnets\u0026#34; { type = list(string) default = [\u0026#34;10.0.101.0/24\u0026#34;, \u0026#34;10.0.102.0/24\u0026#34;, \u0026#34;10.0.103.0/24\u0026#34;] } variable \u0026#34;master_node_count\u0026#34; { type = number default = 3 } variable \u0026#34;worker_node_count\u0026#34; { type = number default = 3 } variable \u0026#34;ssh_user\u0026#34; { type = string default = \u0026#34;ubuntu\u0026#34; } variable \u0026#34;bastion_instance_type\u0026#34; { type = string default = \u0026#34;t3.micro\u0026#34; } variable \u0026#34;master_instance_type\u0026#34; { type = string default = \u0026#34;t3.small\u0026#34; } variable \u0026#34;worker_instance_type\u0026#34; { type = string default = \u0026#34;t3.small\u0026#34; } File 6-key.tf:\nSử dụng Terraform để sinh ra SSH key dùng để SSH vào Bastion Instance resource \u0026#34;tls_private_key\u0026#34; \u0026#34;ssh\u0026#34; { algorithm = \u0026#34;RSA\u0026#34; rsa_bits = 4096 } resource \u0026#34;local_file\u0026#34; \u0026#34;k8_ssh_key\u0026#34; { filename = \u0026#34;k8_ssh_key.pem\u0026#34; file_permission = \u0026#34;600\u0026#34; directory_permission = \u0026#34;700\u0026#34; content = tls_private_key.ssh.private_key_pem } resource \u0026#34;aws_key_pair\u0026#34; \u0026#34;k8_ssh\u0026#34; { key_name = \u0026#34;k8_ssh\u0026#34; public_key = tls_private_key.ssh.public_key_openssh } File 7-outputs.tf:\nresource \u0026#34;tls_private_key\u0026#34; \u0026#34;ssh\u0026#34; { algorithm = \u0026#34;RSA\u0026#34; rsa_bits = 4096 } resource \u0026#34;local_file\u0026#34; \u0026#34;k8_ssh_key\u0026#34; { filename = \u0026#34;k8_ssh_key.pem\u0026#34; file_permission = \u0026#34;600\u0026#34; directory_permission = \u0026#34;700\u0026#34; content = tls_private_key.ssh.private_key_pem } resource \u0026#34;aws_key_pair\u0026#34; \u0026#34;k8_ssh\u0026#34; { key_name = \u0026#34;k8_ssh\u0026#34; public_key = tls_private_key.ssh.public_key_openssh } Apply cấu hình lên AWS Sau khi đã cấu hình các file Terraform, ta sử dụng câu lệnh terraform init và terraform apply để áp dụng các cấu hình tài nguyên lên hạ tầng AWS Kết quả apply thành công "
},
{
	"uri": "//localhost:1313/vi/",
	"title": "Triển khai Kubernetes Cluster trên AWS với Terraform và Ansible",
	"tags": [],
	"description": "",
	"content": "Triển khai HA Kubernetes Cluster trên AWS với Terraform và Ansible Tổng quan Lời đầu tiên, cho phép em/mình xin chào tất cả các anh/chị, các bạn. Đây là bài workshop đầu tiên của em cho nên có thể còn nhiều sai sót mong mọi người bỏ qua, rất mong mọi người có thể tìm thấy những kiến thức có ích, thú vị trong workshop lần này ạ. Em xin cảm ơn.\nTrong bài lab này, mình sẽ thực hiện triển khai một Kubernetes cluster trên môi trường AWS một cách thủ công với kubespray và các máy chủ EC2\nĐể có thể thực hiện bài LAB này, các bạn cần có tài khoản AWS để tạo các máy chủ EC2 và các tài nguyên cần thiết để triển khai Kubernetes Cluster, sau khi hoàn thành bài LAB thì các bạn có thể xóa các tài nguyên trên AWS đi để tránh tốn thêm chi phí.\nSử dụng Terraform để triển khai VPC và các máy chủ EC2 trên AWS Terraform Terraform là một công cụ mã nguồn mở được phát triển bởi HashiCorp, dùng để xây dựng, thay đổi và quản lý các cơ sở hạ tầng trên cloud một cách an toàn và hiệu quả thông qua \u0026ldquo;infrastructure as code\u0026rdquo; (IaC), dịch sát nghĩa thì là \u0026ldquo;quản lý cơ sở hạ tầng dưới dạng mã\u0026rdquo;\nSử dụng Kubespray và Ansible để triển khai Kubernetes Cluster Kubespray Kubespray là một dự án mã nguồn mở của Kubernetes SIGs (Special Interest Groups) dùng để triển khai thủ công các cluster Kubernetes một cách dễ dàng, linh hoạt, và có thể tùy chỉnh trên nhiều môi trường khác nhau. Nó sử dụng Ansible, một công cụ tự động hóa cấu hình, để cài đặt và quản lý Kubernetes clusters. Mặc dù hiện tại có rất nhiều Cloud Provider có cung cấp dịch vụ Kubernetes as a Service như EKS - Elastic Kubernetes Cluster của AWS hay GKE - Google Kubernetes Engine của Google, nhưng với việc sử dụng các dịch vụ này, bạn sẽ không thể có toàn quyền quản lý control plane, \u0026ldquo;đầu não\u0026rdquo; của một Kubernetes Cluster, vì vậy tùy vào mục đích sử dụng và quản lý mà các dự án sử dụng các dịch vụ Kubernetes có sẵn hay tự triển khai và quản lý một hệ thống Kubernetes Cluster riêng. Ansible Ansible là một công cụ mã nguồn mở được sử dụng để tự động hóa các tác vụ quản trị hệ thống, bao gồm cấu hình hệ thống, triển khai phần mềm và quản lý hạ tầng. Ansible được phát triển bởi Red Hat và nổi bật nhờ tính đơn giản, dễ sử dụng và không yêu cầu cài đặt phần mềm đặc biệt trên các máy đích. Nội dung chính Sử dụng Terraform để triển khai Hạ tầng trên AWS Sử dụng Kubespray và Ansible để triển khai Kubernetes Cluster Cấu hình Kubernetes Cluster Dọn dẹp tài nguyên Tài liệu tham khảo "
},
{
	"uri": "//localhost:1313/vi/2-k8s-with-kubespray-ansible/1-bastion-configuration/",
	"title": "Cấu hình Bastion Instance",
	"tags": [],
	"description": "",
	"content": "\rTrong bước này, chúng ta sẽ thực hiện cấu hình Bastion Instance để chuẩn bị cho bước triển khai Kubernetes Cluster, mọi cấu hình liên quan đến các Master Node, Worker Node đều được thực hiện thông qua Bastion Instance.\nCấu hình các Instance Sau khi đã apply xong các file cấu hình Terraform, một file SSH key \u0026ldquo;.pem\u0026rdquo; sẽ được tạo ra trong thư mục chứa các file Terraform, đây chính là key mà chúng ta đã cấu hình để Terraform thực hiện tạo trong file key.tf, và dùng để SSH vào Bastion Instance. Thực hiện SSH vào Bastion Instance để cấu hình Thực hiện cấu hình hostname trên máy Bastion để thuận tiện cho các thao tác SSH cũng như để dễ gợi nhớ hơn, thay vì phải sử dụng IP của các Instance. Cấu hình thêm địa chỉ IP private và tên của các nodes vào file /etc/hosts Cấu hình hostname 127.0.0.1 localhost 10.0.1.114 k8s-master1 10.0.2.151 k8s-master2 10.0.3.26 k8s-master3 10.0.1.98 k8s-worker1 10.0.2.143 k8s-worker2 10.0.3.245 k8s-worker3 # The following lines are desirable for IPv6 capable hosts ::1 ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts Các cấu hình cần thiết để có thể cài đặt Kubernetes Cluster trên các EC2 Instance Sau khi đã SSH được vào Bastion Instance, chúng ta sẽ thực hiện SSH vào các Master và Worker Node để thực hiện cập nhật các package cũng như cấu hình một vài thứ cần thiết trước khi cài đặt K8s Clutser. Để có thể cài đặt Kubernetes Cluster trên các Instance, chúng ta cần phải tắt swap trên các Instance, Tại sao Kubernetes yêu cầu tắt swap? Kubernetes dựa vào khả năng dự đoán và hiệu quả trong quản lý tài nguyên trên các node. Swap có thể gây ra tình trạng trì hoãn không mong muốn khi hệ điều hành quyết định di chuyển dữ liệu từ RAM sang đĩa cứng và ngược lại. Điều này có thể dẫn đến hiệu suất không nhất quán của các pod, làm giảm khả năng quản lý và điều phối tài nguyên của Kubernetes. swapoff -a sudo sed -i \u0026#39;/ swap / s/^/#/\u0026#39; /etc/fstab Cài đặt Kubespray và Ansible Sau khi đã cấu hình thành công cho Bastion Instance, chúng ta đến với bước cài đặt K8s Cluster Như mình đã nói ở trước, tất cả mọi cài đặt sẽ được thực hiện ở Bastion Instance, vì vậy cho nên mình sẽ cài đặt Kubespray và Ansible trên Bastion Instance. sudo apt install git python3 python3-pip -y sudo apt install ansible-core -y Sau khi đã cài đặt thành công Ansible, chúng ta thực hiện clone source code của Kubespray từ Github về để tiến hành cài đặt K8s Cluster. Để có thể cài đặt Kubernetes v1.28 thì các bạn cần chọn đúng phiên bản 2.24 của Kubespray, về phiên bản Kubernetes tương ứng với các phiên bản Kubespray thì các bạn có thể xem ở Document trong Github của Kubespray\ngit clone https://github.com/kubernetes-sigs/kubespray.git --branch release-2.24 "
},
{
	"uri": "//localhost:1313/vi/2-k8s-with-kubespray-ansible/2-kube-spray/",
	"title": "Cài đặt K8s Cluster với Kubespray và Ansible",
	"tags": [],
	"description": "",
	"content": "Nội dung\nCài đặt K8s Cluster với Kubespray và Ansible Nếu bạn không có thiết bị phần cứng , có thể bỏ qua các thao tác dưới đây nhé.\nCài đặt K8s Cluster với Kubespray và Ansible Sau khi đã cài đặt thành công Ansible và Kubespray, chúng ta sẽ thực hiện cài đặt K8s Cluster. Ở thư mục chứa source code của Kubespray, các bạn hãy tạo một file mới hosts.ini trong đường dẫn kubespray/inventory/mycluster/hosts.ini Hiểu nôm na thì file này chứa định nghĩa về các Node, Node nào là Master, Node nào là Worker, địa chỉ IP của nó là bao nhiêu,\u0026hellip; File host này cũng đã có file mẫu trên Github của Kubespray, các bạn có thể lấy về và chỉnh sửa tùy theo cấu hình của mình, ở dưới là file hosts.ini của mình, mọi người có thể xem qua và tham khảo. ## Configure \u0026#39;ip\u0026#39; variable to bind kubernetes services on a ## different ip than the default iface k8s-master1 ansible_host=10.0.1.181 ip=10.0.1.181 k8s-master2 ansible_host=10.0.2.7 ip=10.0.2.7 k8s-master3 ansible_host=10.0.3.17 ip=10.0.3.17 k8s-worker1 ansible_host=10.0.1.95 ip=10.0.1.95 k8s-worker2 ansible_host=10.0.2.10 ip=10.0.2.10 k8s-worker3 ansible_host=10.0.3.69 ip=10.0.3.69 [kube_control_plane] k8s-master1 k8s-master2 k8s-master3 [etcd] k8s-master1 k8s-master2 k8s-master3 [kube_node] k8s-worker1 k8s-worker2 k8s-worker3 [k8s_cluster:children] kube_node kube_master kube_control_plane [calico-rr] [vault] k8s-master1 Tiếp theo chúng ta sẽ sử dụng Ansible để bắt đầu quá trình cài đặt K8s-Cluster ansible-playbook -i inventory/mycluster/hosts.ini --become --become-user=root cluster.yml Quá trình cài đặt có thể kéo dài từ 15-20p\nKết quả sau khi cài đặt thành công Tiếp theo chúng ta cần cấu hình kubectl trước khi có thể bắt đầu vào sử dụng K8s Cluster\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Cấu hình kubectl Vậy là chúng ta đã cài đặt thành công một cụm Kubernetes cluster với Kubespray và Ansible, với việc triển khai thủ công theo cách này thì việc chúng ta thêm, xóa, sửa các node trong cluster rất dễ dàng tùy thuộc vào nhu cầu và mục đích sử dụng của từng thời điểm.\n"
},
{
	"uri": "//localhost:1313/vi/2-k8s-with-kubespray-ansible/",
	"title": "Sử dụng Kubespray và Ansible để triển khai Kubernetes Cluster",
	"tags": [],
	"description": "",
	"content": "Trong bước ngày, chúng ta sẽ thực hiện cấu hình Bastion Instance cũng như sử dụng Kubespray và Ansible để cài đặt Kubernetes Cluster\nNội Dung Cấu hình Bastion Instance Cài đặt K8s Cluster với Kubespray và Ansible "
},
{
	"uri": "//localhost:1313/vi/3-clean-up/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Dọn dẹp tài nguyên Sau khi đã \u0026ldquo;vọc vạch\u0026rdquo; xong xuôi thì các bạn chỉ cần một lệnh duy nhất để có thể dọn đẹp hết tất cả tài nguyên trên AWS, đây cũng là một trong những lợi ích mà mình thích nhất khi sử dụng Terraform Bài Workshop đầu tay của mình đến đây là kết thúc, rất cảm ơn mọi người đã đọc đến đây và hẹn mọi người ở các Workshop tiếp theo ^^. Tài liệu tham khảo Kubespray HA Kubernetes Cluster in 15 Minutes "
},
{
	"uri": "//localhost:1313/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]