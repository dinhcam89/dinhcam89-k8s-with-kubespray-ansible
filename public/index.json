[
{
	"uri": "//localhost:1313/1-terraform-for-infra/",
	"title": "Using Terraform to Deploy Infrastructure on AWS",
	"tags": [],
	"description": "",
	"content": "Contents:\nStructure of Terraform Configuration Files Configuration files Apply cấu hình lên AWS Structure of Terraform Configuration Files Since this LAB focuses only on manually deploying a Kubernetes Cluster, I won\u0026rsquo;t go into detail about Terraform or Ansible. If you want to learn more about these tools, you can find many excellent and detailed guides on sites like Medium or Viblo VN.\nterraform/ │ ├── 0-provider.tf # Configuration file to define the Cloud Provider you will use Terraform to deploy ├── 1-vpc.tf # Defines the configuration for AWS VPC ├── 2-instances.tf # Defines the configuration for AWS EC2 Instances ├── 3-security-groups.tf # Defines security groups ├── 4-load-balancers.tf # Configures Load Balancing for Master Nodes ├── 5-vars.tf # Defines variables used in .tf files ├── 6-key.tf # Configures the creation of SSH key for SSH access to Bastion Instance ├── 7-output.tf # Configuration for output after apply Configuration files File 0-provider.tf:\nThis file defines the Cloud Provider (in this case AWS) with which Terraform will interact with its resources. It sets up the required provider and specifies the region (region) of AWS where the resources will be exploited. provider \u0026#34;aws\u0026#34; { region = var.region } terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } } File 1-vpc.tf:\nThis is a Terraform file used to deploy a VPC, which stands for \u0026ldquo;Virtual Private Cloud\u0026rdquo; on the AWS environment. With Terraform, you can define the components of a VPC such as network, subnets, internet gateway, NAT gateway, security groups, etc. However, in this LAB, I will use a publicly available module from the Terraform Registry and only need to adjust the attribute values according to the module\u0026rsquo;s documentation to customize the VPC as needed. For example: single_nat_gateway = true: means that all subnets will use a single NAT gateway; by default, this module will create one NAT gateway per subnet. cidr: Defines the CIDR block for the VPC, which is the IP address range that the VPC can use. For example: \u0026ldquo;10.0.0.0/16\u0026rdquo; allows you to use IP addresses from 10.0.0.0 to 10.0.255.255. azs: List of Availability Zones where you will deploy subnets in the VPC (such as us-east-1a, us-east-1b, us-east-1c,\u0026hellip;). This ensures high availability and load balancing for the resources. Other attributes and options can be found in the documentation of each module on the Terraform Registry module \u0026#34;vpc\u0026#34; { source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; name = \u0026#34;self-managed-k8s-vpc\u0026#34; cidr = \u0026#34;10.0.0.0/16\u0026#34; azs = var.availability_zones private_subnets = var.private_subnets public_subnets = var.public_subnets enable_nat_gateway = true single_nat_gateway = true one_nat_gateway_per_az = false map_public_ip_on_launch = true //assign public ip for BASTION instance tags = { Terraform = \u0026#34;true\u0026#34; Environment = \u0026#34;dev\u0026#34; } } File 2-instances.tf:\nMake sure to create instances with at least 1 CPU và 2GB RAM otherwise, you may encounter resource errors during the K8s installation process.\nThis file will define the configurations related to instances, including the Bastion Instance, Master Nodes and Worker Nodes For example: ami_id :lThe ID of the \u0026ldquo;image\u0026rdquo; that your EC2 instances will use, such as Linux, Ubuntu, CentOS, etc. instance_type :The configuration of the EC2 instances you want to deploy, such as t2.micro, t2.small, t3.medium, t3.large,\u0026hellip; subnet_id :The subnet ID where the EC2 instance will be located. security_groups :The IDs of the security groups that need to be assigned to the instance. key_name :The name of the SSH key to be imported into the instance. user_data :c :Configuration to import the SSH key into the instance. In this LAB, I use Terraform to generate an SSH key and import it into the Bastion Instance. In practice, you can also create a key on AWS and assign it to the instance or create a key on your local machine and then import the public key into the instance\u0026rsquo;s user data after successfully deploying the infrastructure with Terraform.\n// Bastion Instance resource \u0026#34;aws_instance\u0026#34; \u0026#34;bastion\u0026#34; { ami = var.ami_id instance_type = var.bastion_instance_type subnet_id = module.vpc.public_subnets[0] associate_public_ip_address = \u0026#34;true\u0026#34; security_groups = [aws_security_group.allow_ssh.id] key_name = aws_key_pair.k8_ssh.key_name user_data = \u0026lt;\u0026lt;-EOF #!bin/bash echo \u0026#34;PubkeyAcceptedKeyTypes=+ssh-rsa\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config.d/10-insecure-rsa-keysig.conf systemctl reload sshd echo \u0026#34;${tls_private_key.ssh.private_key_pem}\u0026#34; \u0026gt;\u0026gt; /home/ubuntu/.ssh/id_rsa chown ubuntu /home/ubuntu/.ssh/id_rsa chgrp ubuntu /home/ubuntu/.ssh/id_rsa chmod 600 /home/ubuntu/.ssh/id_rsa echo \u0026#34;starting ansible install\u0026#34; apt-add-repository ppa:ansible/ansible -y apt update apt install ansible -y EOF tags = { Name = \u0026#34;Bastion\u0026#34; } } // Master Nodes resource \u0026#34;aws_instance\u0026#34; \u0026#34;masters\u0026#34; { count = var.master_node_count ami = var.ami_id instance_type = var.master_instance_type subnet_id = \u0026#34;${element(module.vpc.private_subnets, count.index)}\u0026#34; key_name = aws_key_pair.k8_ssh.key_name security_groups = [aws_security_group.k8_nondes.id, aws_security_group.k8_masters.id] tags = { Name = format(\u0026#34;k8s-master-%02d\u0026#34;, count.index + 1) } } // Worker Nodes resource \u0026#34;aws_instance\u0026#34; \u0026#34;workers\u0026#34; { count = var.worker_node_count ami = var.ami_id instance_type = var.worker_instance_type subnet_id = \u0026#34;${element(module.vpc.private_subnets, count.index)}\u0026#34; key_name = aws_key_pair.k8_ssh.key_name security_groups = [aws_security_group.k8_nondes.id, aws_security_group.k8_workers.id] tags = { Name = format(\u0026#34;k8s-worker-%02d\u0026#34;, count.index + 1) } } File 3-security_groups.tf:\nThis section will configure the Security Groups needed for the VPC: SG to allow SSH access to the Bastion Instance SGs required for the Kubernetes Cluster, master nodes, and worker nodes resource \u0026#34;aws_security_group\u0026#34; \u0026#34;allow_ssh\u0026#34; { name = \u0026#34;allow_ssh\u0026#34; description = \u0026#34;Allow ssh inbound traffic\u0026#34; vpc_id = module.vpc.vpc_id ingress { from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } ingress { from_port = -1 to_port = -1 protocol = \u0026#34;icmp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;k8_nondes\u0026#34; { name = \u0026#34;k8_nodes\u0026#34; description = \u0026#34;sec group for k8 nodes\u0026#34; vpc_id = module.vpc.vpc_id ingress { from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } ingress { from_port = -1 to_port = -1 protocol = \u0026#34;icmp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;k8_masters\u0026#34; { name = \u0026#34;k8_masters\u0026#34; description = \u0026#34;sec group for k8 master nodes\u0026#34; vpc_id = module.vpc.vpc_id ingress { #Kubernetes API server from_port = 6443 to_port = 6443 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } ingress { #etcd server client API from_port = 2379 to_port = 2380 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } ingress { #Kubelet API from_port = 10250 to_port = 10250 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } ingress { #kube-scheduler from_port = 10259 to_port = 10259 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } ingress { #kube-controller-manager from_port = 10257 to_port = 10257 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;k8_workers\u0026#34; { name = \u0026#34;k8_workers\u0026#34; description = \u0026#34;sec group for k8 worker nodes\u0026#34; vpc_id = module.vpc.vpc_id ingress { #Kubelet API from_port = 10250 to_port = 10250 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } ingress { #NodePort Services† from_port = 30000 to_port = 32767 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.vpc_cidr}\u0026#34;] } } File 4-load-balancers.tf:\nThis file defines resources related to Load Balancer (LB) for Master Nodes in the Kubernetes Cluster on AWS. If you want to save costs on making LAB, you can use only Master and skip this LoadBalancer file. resource \u0026#34;aws_lb\u0026#34; \u0026#34;k8_masters_lb\u0026#34; { name = \u0026#34;k8-masters-lb\u0026#34; internal = true load_balancer_type = \u0026#34;network\u0026#34; subnets = module.vpc.private_subnets #[for subnet in module.vpc.private_subnets : subnet.id] tags = { Terraform = \u0026#34;true\u0026#34; Environment = \u0026#34;dev\u0026#34; } } resource \u0026#34;aws_lb_target_group\u0026#34; \u0026#34;k8_masters_api\u0026#34; { name = \u0026#34;k8-masters-api\u0026#34; port = 6443 protocol = \u0026#34;TCP\u0026#34; vpc_id = module.vpc.vpc_id target_type = \u0026#34;ip\u0026#34; health_check { port = 6443 protocol = \u0026#34;TCP\u0026#34; interval = 30 healthy_threshold = 2 unhealthy_threshold = 2 } } resource \u0026#34;aws_lb_listener\u0026#34; \u0026#34;k8_masters_lb_listener\u0026#34; { load_balancer_arn = aws_lb.k8_masters_lb.arn port = 6443 protocol = \u0026#34;TCP\u0026#34; default_action { target_group_arn = aws_lb_target_group.k8_masters_api.id type = \u0026#34;forward\u0026#34; } } resource \u0026#34;aws_lb_target_group_attachment\u0026#34; \u0026#34;k8_masters_attachment\u0026#34; { count = length(aws_instance.masters.*.id) target_group_arn = aws_lb_target_group.k8_masters_api.arn target_id = aws_instance.masters.*.private_ip[count.index] } File 5-vars.tf:\nThis file defines the variables that we will pass into other .tf files. The purpose is to make it easier to change and edit these parameters, instead of everyone having to search for each place containing the variable \u0026ldquo;x \u0026quot; To change the value, we only need to change it in one place in this \u0026ldquo;var.tf\u0026rdquo; file. variable \u0026#34;region\u0026#34; { default = \u0026#34;us-east-1\u0026#34; } variable \u0026#34;ami_id\u0026#34; { type = string default = \u0026#34;ami-04a81a99f5ec58529\u0026#34; // Ubuntu 24.04 LTS } variable \u0026#34;availability_zones\u0026#34; { type = list(string) default = [\u0026#34;us-east-1a\u0026#34;, \u0026#34;us-east-1b\u0026#34;, \u0026#34;us-east-1c\u0026#34;] } variable \u0026#34;vpc_cidr\u0026#34; { type = string default = \u0026#34;10.0.0.0/16\u0026#34; } variable \u0026#34;private_subnets\u0026#34; { type = list(string) default = [\u0026#34;10.0.1.0/24\u0026#34;, \u0026#34;10.0.2.0/24\u0026#34;, \u0026#34;10.0.3.0/24\u0026#34;] } variable \u0026#34;public_subnets\u0026#34; { type = list(string) default = [\u0026#34;10.0.101.0/24\u0026#34;, \u0026#34;10.0.102.0/24\u0026#34;, \u0026#34;10.0.103.0/24\u0026#34;] } variable \u0026#34;master_node_count\u0026#34; { type = number default = 3 } variable \u0026#34;worker_node_count\u0026#34; { type = number default = 3 } variable \u0026#34;ssh_user\u0026#34; { type = string default = \u0026#34;ubuntu\u0026#34; } variable \u0026#34;bastion_instance_type\u0026#34; { type = string default = \u0026#34;t3.micro\u0026#34; } variable \u0026#34;master_instance_type\u0026#34; { type = string default = \u0026#34;t3.small\u0026#34; } variable \u0026#34;worker_instance_type\u0026#34; { type = string default = \u0026#34;t3.small\u0026#34; } File 6-key.tf:\nUse Terraform to generate an SSH key for SSH access to the Bastion Instance. resource \u0026#34;tls_private_key\u0026#34; \u0026#34;ssh\u0026#34; { algorithm = \u0026#34;RSA\u0026#34; rsa_bits = 4096 } resource \u0026#34;local_file\u0026#34; \u0026#34;k8_ssh_key\u0026#34; { filename = \u0026#34;k8_ssh_key.pem\u0026#34; file_permission = \u0026#34;600\u0026#34; directory_permission = \u0026#34;700\u0026#34; content = tls_private_key.ssh.private_key_pem } resource \u0026#34;aws_key_pair\u0026#34; \u0026#34;k8_ssh\u0026#34; { key_name = \u0026#34;k8_ssh\u0026#34; public_key = tls_private_key.ssh.public_key_openssh } File 7-outputs.tf:\nresource \u0026#34;tls_private_key\u0026#34; \u0026#34;ssh\u0026#34; { algorithm = \u0026#34;RSA\u0026#34; rsa_bits = 4096 } resource \u0026#34;local_file\u0026#34; \u0026#34;k8_ssh_key\u0026#34; { filename = \u0026#34;k8_ssh_key.pem\u0026#34; file_permission = \u0026#34;600\u0026#34; directory_permission = \u0026#34;700\u0026#34; content = tls_private_key.ssh.private_key_pem } resource \u0026#34;aws_key_pair\u0026#34; \u0026#34;k8_ssh\u0026#34; { key_name = \u0026#34;k8_ssh\u0026#34; public_key = tls_private_key.ssh.public_key_openssh } Apply cấu hình lên AWS After configuring the Terraform files, use the commands terraform init and terraform apply to apply the resource configurations to the AWS infrastructure. Successful apply results\n"
},
{
	"uri": "//localhost:1313/",
	"title": "Deploying a Kubernetes Cluster on AWS with Terraform and Ansible",
	"tags": [],
	"description": "",
	"content": "Deploying a HA Kubernetes Cluster on AWS with Terraform and Ansible Overview First of all, let me say hello to all of you. This is my first workshop, so there might be some mistakes, and I hope you can overlook them. I hope you will find useful and interesting knowledge in this workshop. Thank you.\nIn this lab, we will manually deploy a Kubernetes cluster on AWS using kubespray and EC2 instances.\nTo perform this LAB, you need an AWS account to create EC2 instances and the necessary resources to deploy the Kubernetes Cluster. After completing the LAB, you can delete these resources on AWS to avoid additional costs.\nUsing Terraform to Deploy VPC and EC2 Instances on AWS Terraform is an open-source tool developed by HashiCorp, used to build, change, and manage infrastructure on the cloud safely and efficiently through \u0026ldquo;infrastructure as code\u0026rdquo; (IaC).\nUsing Kubespray and Ansible to Deploy a Kubernetes Cluster Kubespray Kubespray is an open-source project by Kubernetes SIGs (Special Interest Groups) used to deploy Kubernetes clusters manually, easily, flexibly, and customizable on various environments. It uses Ansible, a configuration automation tool, to install and manage Kubernetes clusters. Although there are many cloud providers offering Kubernetes as a Service, like AWS\u0026rsquo;s EKS (Elastic Kubernetes Service) or Google\u0026rsquo;s GKE (Google Kubernetes Engine), using these services does not give you full control over the control plane, the \u0026ldquo;brain\u0026rdquo; of a Kubernetes Cluster. Depending on your usage and management needs, you may choose to use these ready-made Kubernetes services or deploy and manage your own Kubernetes Cluster. Ansible Ansible is an open-source tool used to automate system administration tasks, including system configuration, software deployment, and infrastructure management. Developed by Red Hat, Ansible is known for its simplicity, ease of use, and does not require special software installation on target machines. Main Contents Using Terraform to Deploy Infrastructure on AWS Using Kubespray and Ansible to Deploy a Kubernetes Cluster Cleaning UP Resource "
},
{
	"uri": "//localhost:1313/2-k8s-with-kubespray-ansible/1-bastion-configuration/",
	"title": "Configuring the Bastion Instance",
	"tags": [],
	"description": "",
	"content": "\rIn this step, we will configure the Bastion Instance to prepare for deploying the Kubernetes Cluster. All configurations related to Master Nodes and Worker Nodes will be done through the Bastion Instance.\nConfiguring the Instances After applying the Terraform configuration files, an SSH key file \u0026ldquo;.pem\u0026rdquo; will be created in the directory containing the Terraform files. This key is the one we configured in the key.tf file for Terraform to create, and it will be used to SSH into the Bastion Instance. SSH into the Bastion Instance for configuration. Configure the hostname on the Bastion machine to simplify SSH operations and make it easier to remember, rather than using the IP addresses of the Instances. Add the private IP addresses and names of the nodes to the /etc/hosts file. Configure hostname 127.0.0.1 localhost p 10.0.1.114 k8s-master1 10.0.2.151 k8s-master2 10.0.3.26 k8s-master3 10.0.1.98 k8s-worker1 10.0.2.143 k8s-worker2 10.0.3.245 k8s-worker3 # The following lines are desirable for IPv6 capable hosts ::1 ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts Necessary configurations for installing the Kubernetes Cluster on EC2 Instances After SSH access to the Bastion Instance, SSH into the Master and Worker Nodes to update packages and configure a few things needed before installing the Kubernetes Cluster. To install the Kubernetes Cluster on the Instances, we need to disable swap on the Instances. Why does Kubernetes require swap to be disabled? Kubernetes relies on predictable and efficient resource management on nodes. Swap can cause undesirable delays when the operating system decides to move data between RAM and disk. This can lead to inconsistent pod performance, reducing Kubernetes\u0026rsquo; ability to manage and coordinate resources effectively. swapoff -a sudo sed -i \u0026#39;/ swap / s/^/#/\u0026#39; /etc/fstab Installing Kubespray and Ansible After successfully configuring the Bastion Instance, we move on to installing the K8s Cluster. As mentioned earlier, all installations will be done on the Bastion Instance, so we will install Kubespray and Ansible on the Bastion Instance. sudo apt install git python3 python3-pip -y sudo apt install ansible-core -y After successfully installing Ansible, clone the source code của Kubespray from GitHub to proceed with the K8s Cluster installation. To install Kubernetes v1.28, you need to select the correct version 2.24 of Kubespray. For information on the Kubernetes versions corresponding to Kubespray versions, refer to the documentation on the Github của Kubespray\ngit clone https://github.com/kubernetes-sigs/kubespray.git --branch release-2.24 "
},
{
	"uri": "//localhost:1313/2-k8s-with-kubespray-ansible/2-kube-spray/",
	"title": "Installing the K8s Cluster with Kubespray and Ansible",
	"tags": [],
	"description": "",
	"content": "Content\nInstalling the K8s Cluster with Kubespray and Ansible Installing the K8s Cluster with Kubespray and Ansible After successfully installing Ansible and Kubespray, we will proceed with installing the K8s Cluster. In the directory containing the Kubespray source code, create a new file named hosts.ini in the path kubespray/inventory/mycluster/hosts.ini Essentially, this file contains definitions of the Nodes, which nodes are Master, which nodes are Worker, their IP addresses, etc. -This host file also has a sample file available on the Kubespray\u0026rsquo;s Github, which you can download and modify according to your configuration. Below is my hosts.ini file for reference: ## Configure \u0026#39;ip\u0026#39; variable to bind kubernetes services on a ## different ip than the default iface k8s-master1 ansible_host=10.0.1.181 ip=10.0.1.181 k8s-master2 ansible_host=10.0.2.7 ip=10.0.2.7 k8s-master3 ansible_host=10.0.3.17 ip=10.0.3.17 k8s-worker1 ansible_host=10.0.1.95 ip=10.0.1.95 k8s-worker2 ansible_host=10.0.2.10 ip=10.0.2.10 k8s-worker3 ansible_host=10.0.3.69 ip=10.0.3.69 [kube_control_plane] k8s-master1 k8s-master2 k8s-master3 [etcd] k8s-master1 k8s-master2 k8s-master3 [kube_node] k8s-worker1 k8s-worker2 k8s-worker3 [k8s_cluster:children] kube_node kube_master kube_control_plane [calico-rr] [vault] k8s-master1 Next, we will use Ansible to start the process of installing the K8s Cluster: bash ansible-playbook -i inventory/mycluster/hosts.ini --become --become-user=root cluster.yml The installation process may take 15-20 minutes.\nResult after successful installation: Next, we need to configure kubectl before we can start using the K8s Cluster:\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Configuring kubectl: And that\u0026rsquo;s it! We have successfully installed a Kubernetes cluster with Kubespray and Ansible. With this manual deployment method, adding, removing, or modifying nodes in the cluster is quite straightforward, depending on the needs and objectives at any given time.\n"
},
{
	"uri": "//localhost:1313/2-k8s-with-kubespray-ansible/",
	"title": "Using Kubespray and Ansible to Deploy a Kubernetes Cluster",
	"tags": [],
	"description": "",
	"content": "In this section, we will configure the Bastion Instance and use Kubespray and Ansible to install the Kubernetes Cluster.\nContents Configuring the Bastion Instance Installing K8s Cluster with Kubespray and Ansible "
},
{
	"uri": "//localhost:1313/3-clean-up/",
	"title": "Cleaning Up Resources",
	"tags": [],
	"description": "",
	"content": "Cleaning Up Resources After you\u0026rsquo;ve finished experimenting, you just need a single command to clean up all resources on AWS. This is one of the features I appreciate most about using Terraform. Terraform Destroy This concludes my first workshop. Thank you all for reading, and I look forward to seeing you in the next workshops ^^. References Kubespray HA Kubernetes Cluster in 15 Minutes "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]